{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import jsonlines\n",
    "import warnings\n",
    "import random\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "\n",
    "import spacy\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_tokenizer(nlp):\n",
    "    prefix_re = spacy.util.compile_prefix_regex(tuple([r'-', r'\\d{2}\\.\\d{2}\\.\\d{4}'] + list(nlp.Defaults.prefixes)))\n",
    "    infix_re = spacy.util.compile_infix_regex(tuple([r'(\\.)', r'(:)', r'(\\()', r'(\\))'] + list(nlp.Defaults.infixes)))\n",
    "    suffixes = list(nlp.Defaults.suffixes)\n",
    "    suffixes.remove('\\.\\.+')\n",
    "    suffixes.append('\\.\\.\\.+')\n",
    "    suffixes.append('Die')\n",
    "    suffix_re = spacy.util.compile_suffix_regex(tuple([r'-'] + suffixes))\n",
    "    return Tokenizer(nlp.vocab, nlp.Defaults.tokenizer_exceptions,\n",
    "                     prefix_search = prefix_re.search, \n",
    "                     infix_finditer = infix_re.finditer,\n",
    "                     suffix_search = suffix_re.search,\n",
    "                     token_match=None)\n",
    "\n",
    "def evaluate(ner_model, examples):\n",
    "    scorer = Scorer()\n",
    "    for input_, annot in examples:\n",
    "        doc_gold_text = ner_model.make_doc(input_)\n",
    "        gold = GoldParse(doc_gold_text, entities=annot)\n",
    "        pred_value = ner_model(input_)\n",
    "        scorer.score(pred_value, gold)\n",
    "    return scorer.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary = re.compile('^[0-9]$')\n",
    "\n",
    "\n",
    "def custom_seg(doc):\n",
    "    prev = doc[0].text\n",
    "    length = len(doc)\n",
    "    for index, token in enumerate(doc):\n",
    "        is_number = token.text == '.' and boundary.match(prev) and index != (length - 1)\n",
    "        if is_number or token.text in [':', ';', ',', '/', '*'] or not token.is_punct:\n",
    "            next_t = index + 1\n",
    "            while next_t < length:\n",
    "                doc[next_t].sent_start = False\n",
    "                if doc[next_t].is_space:\n",
    "                    next_t += 1\n",
    "                else:\n",
    "                    break\n",
    "        prev = token.text\n",
    "    return doc\n",
    "\n",
    "\n",
    "CUSTOM_SEG = 'custom_seg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open('dataset.jsonl') as reader:\n",
    "    data = [obj for obj in reader]\n",
    "\n",
    "train_data = [(row['text'], {'entities': row['labels']}) for row in data[:150]]\n",
    "test_data = [(row['text'], row['labels']) for row in data[150:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'custom_seg', 'parser', 'ner']"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('de_core_news_lg')\n",
    "if CUSTOM_SEG in nlp.pipe_names:\n",
    "    nlp.remove_pipe(CUSTOM_SEG)\n",
    "nlp.add_pipe(custom_seg, name=CUSTOM_SEG, before='parser')\n",
    "\n",
    "nlp.tokenizer = create_custom_tokenizer(nlp)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(ner)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "for _, annotations in train_data:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nlp.resume_training()\n",
    "move_names = list(ner.move_names)\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]2020-12-29 20:41:58.850 | INFO     | __main__:<module>:13 - Losses: {'ner': 5521.989167222448}\n",
      "  3%|▎         | 1/30 [00:12<06:03, 12.54s/it]2020-12-29 20:42:10.725 | INFO     | __main__:<module>:13 - Losses: {'ner': 5074.476464488759}\n",
      "  7%|▋         | 2/30 [00:24<05:45, 12.33s/it]2020-12-29 20:42:22.641 | INFO     | __main__:<module>:13 - Losses: {'ner': 4665.722264235562}\n",
      " 10%|█         | 3/30 [00:36<05:29, 12.20s/it]2020-12-29 20:42:34.743 | INFO     | __main__:<module>:13 - Losses: {'ner': 4692.655696151522}\n",
      " 13%|█▎        | 4/30 [00:48<05:16, 12.17s/it]2020-12-29 20:42:45.000 | INFO     | __main__:<module>:13 - Losses: {'ner': 5125.704040696715}\n",
      " 17%|█▋        | 5/30 [00:58<04:49, 11.60s/it]2020-12-29 20:42:52.884 | INFO     | __main__:<module>:13 - Losses: {'ner': 4448.686472449401}\n",
      " 20%|██        | 6/30 [01:06<04:11, 10.48s/it]2020-12-29 20:43:00.972 | INFO     | __main__:<module>:13 - Losses: {'ner': 4662.792940271889}\n",
      " 23%|██▎       | 7/30 [01:14<03:44,  9.76s/it]2020-12-29 20:43:09.120 | INFO     | __main__:<module>:13 - Losses: {'ner': 4652.445964402501}\n",
      " 27%|██▋       | 8/30 [01:22<03:24,  9.29s/it]2020-12-29 20:43:17.137 | INFO     | __main__:<module>:13 - Losses: {'ner': 7239.931698700329}\n",
      " 30%|███       | 9/30 [01:30<03:06,  8.90s/it]2020-12-29 20:43:25.429 | INFO     | __main__:<module>:13 - Losses: {'ner': 4238.502407309844}\n",
      " 33%|███▎      | 10/30 [01:39<02:54,  8.71s/it]2020-12-29 20:43:32.226 | INFO     | __main__:<module>:13 - Losses: {'ner': 6008.912093421331}\n",
      " 37%|███▋      | 11/30 [01:45<02:34,  8.14s/it]2020-12-29 20:43:39.048 | INFO     | __main__:<module>:13 - Losses: {'ner': 4573.411630091854}\n",
      " 40%|████      | 12/30 [01:52<02:19,  7.74s/it]2020-12-29 20:43:46.027 | INFO     | __main__:<module>:13 - Losses: {'ner': 4917.080353771313}\n",
      " 43%|████▎     | 13/30 [01:59<02:07,  7.53s/it]2020-12-29 20:43:53.089 | INFO     | __main__:<module>:13 - Losses: {'ner': 4676.027171333168}\n",
      " 47%|████▋     | 14/30 [02:06<01:58,  7.38s/it]2020-12-29 20:43:59.905 | INFO     | __main__:<module>:13 - Losses: {'ner': 4161.979364339527}\n",
      " 50%|█████     | 15/30 [02:13<01:48,  7.21s/it]2020-12-29 20:44:06.554 | INFO     | __main__:<module>:13 - Losses: {'ner': 4186.525612986431}\n",
      " 53%|█████▎    | 16/30 [02:20<01:38,  7.04s/it]2020-12-29 20:44:12.847 | INFO     | __main__:<module>:13 - Losses: {'ner': 6836.002272173559}\n",
      " 57%|█████▋    | 17/30 [02:26<01:28,  6.82s/it]2020-12-29 20:44:19.473 | INFO     | __main__:<module>:13 - Losses: {'ner': 4495.022993783359}\n",
      " 60%|██████    | 18/30 [02:33<01:21,  6.77s/it]2020-12-29 20:44:26.191 | INFO     | __main__:<module>:13 - Losses: {'ner': 4239.462239747806}\n",
      " 63%|██████▎   | 19/30 [02:39<01:14,  6.74s/it]2020-12-29 20:44:32.317 | INFO     | __main__:<module>:13 - Losses: {'ner': 3958.336089982855}\n",
      " 67%|██████▋   | 20/30 [02:45<01:05,  6.56s/it]2020-12-29 20:44:38.638 | INFO     | __main__:<module>:13 - Losses: {'ner': 5357.837623490494}\n",
      " 70%|███████   | 21/30 [02:52<00:58,  6.49s/it]2020-12-29 20:44:44.568 | INFO     | __main__:<module>:13 - Losses: {'ner': 5247.975904923285}\n",
      " 73%|███████▎  | 22/30 [02:58<00:50,  6.32s/it]2020-12-29 20:44:50.470 | INFO     | __main__:<module>:13 - Losses: {'ner': 6674.036310788244}\n",
      " 77%|███████▋  | 23/30 [03:04<00:43,  6.20s/it]2020-12-29 20:44:56.269 | INFO     | __main__:<module>:13 - Losses: {'ner': 4251.0351244675985}\n",
      " 80%|████████  | 24/30 [03:09<00:36,  6.09s/it]2020-12-29 20:45:02.358 | INFO     | __main__:<module>:13 - Losses: {'ner': 4003.9594585965187}\n",
      " 83%|████████▎ | 25/30 [03:15<00:30,  6.07s/it]2020-12-29 20:45:08.212 | INFO     | __main__:<module>:13 - Losses: {'ner': 4047.315825334146}\n",
      " 87%|████████▋ | 26/30 [03:21<00:24,  6.01s/it]2020-12-29 20:45:14.124 | INFO     | __main__:<module>:13 - Losses: {'ner': 4274.609087430119}\n",
      " 90%|█████████ | 27/30 [03:27<00:17,  5.98s/it]2020-12-29 20:45:21.037 | INFO     | __main__:<module>:13 - Losses: {'ner': 6134.45235171658}\n",
      " 93%|█████████▎| 28/30 [03:35<00:12,  6.37s/it]2020-12-29 20:45:28.308 | INFO     | __main__:<module>:13 - Losses: {'ner': 5389.018093335442}\n",
      " 97%|█████████▋| 29/30 [03:42<00:06,  6.58s/it]2020-12-29 20:45:35.198 | INFO     | __main__:<module>:13 - Losses: {'ner': 4627.2145929038525}\n",
      "100%|██████████| 30/30 [03:48<00:00,  7.63s/it]\n"
     ]
    }
   ],
   "source": [
    "with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "\n",
    "    warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "    sizes = compounding(1.0, 16.0, 1.001)\n",
    "\n",
    "    for _ in tqdm(range(30)):\n",
    "        random.shuffle(train_data)\n",
    "        batches = minibatch(train_data, size=sizes)\n",
    "        losses = {}\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n",
    "        logger.info(f\"Losses: {losses}\")\n",
    "            \n",
    "        scores.append(evaluate(nlp, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0 100.0 100.0\n"
     ]
    }
   ],
   "source": [
    "#last iteration score\n",
    "index = -1\n",
    "print(scores[index]['ents_p'], scores[index]['ents_r'], scores[index]['ents_f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uas': 0.0,\n",
       " 'las': 0.0,\n",
       " 'las_per_type': {'': {'p': 0.0, 'r': 0.0, 'f': 0.0}},\n",
       " 'ents_p': 100.0,\n",
       " 'ents_r': 100.0,\n",
       " 'ents_f': 100.0,\n",
       " 'ents_per_type': {'COMPANY_ADDRESS': {'p': 100.0, 'r': 100.0, 'f': 100.0},\n",
       "  'COMPANY_NAME': {'p': 100.0, 'r': 100.0, 'f': 100.0}},\n",
       " 'tags_acc': 0.0,\n",
       " 'token_acc': 100.0,\n",
       " 'textcat_score': 0.0,\n",
       " 'textcats_per_cat': {}}"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model\n",
    "nlp.meta['name'] = 'Registration Docs Parser'\n",
    "nlp.meta['version'] = '1'\n",
    "nlp.remove_pipe(CUSTOM_SEG)\n",
    "nlp.to_disk('model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model\n",
    "nlp = spacy.load('model/')\n",
    "if CUSTOM_SEG in nlp.pipe_names:\n",
    "    nlp.remove_pipe(CUSTOM_SEG)\n",
    "nlp.add_pipe(custom_seg, name=CUSTOM_SEG, before='parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.48991354466858 92.3529411764706 91.41193595342067\n"
     ]
    }
   ],
   "source": [
    "## test data on independent data\n",
    "scorer_test = [[x['text'], x['labels']] for x in data[30:]]\n",
    "score = evaluate(nlp, scorer_test)\n",
    "print(score['ents_p'], score['ents_r'], score['ents_f'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
