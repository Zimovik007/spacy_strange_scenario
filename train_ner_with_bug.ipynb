{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "import jsonlines\n",
    "import warnings\n",
    "import random\n",
    "from loguru import logger\n",
    "from tqdm import tqdm\n",
    "\n",
    "import spacy\n",
    "from spacy.scorer import Scorer\n",
    "from spacy.gold import GoldParse\n",
    "from spacy.tokenizer import Tokenizer\n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_custom_tokenizer(nlp):\n",
    "    prefix_re = spacy.util.compile_prefix_regex(tuple([r'-', r'\\d{2}\\.\\d{2}\\.\\d{4}'] + list(nlp.Defaults.prefixes)))\n",
    "    infix_re = spacy.util.compile_infix_regex(tuple([r'(\\.)', r'(:)', r'(\\()', r'(\\))'] + list(nlp.Defaults.infixes)))\n",
    "    suffixes = list(nlp.Defaults.suffixes)\n",
    "    suffixes.remove('\\.\\.+')\n",
    "    suffixes.append('\\.\\.\\.+')\n",
    "    suffixes.append('Die')\n",
    "    suffix_re = spacy.util.compile_suffix_regex(tuple([r'-'] + suffixes))\n",
    "    return Tokenizer(nlp.vocab, nlp.Defaults.tokenizer_exceptions,\n",
    "                     prefix_search = prefix_re.search, \n",
    "                     infix_finditer = infix_re.finditer,\n",
    "                     suffix_search = suffix_re.search,\n",
    "                     token_match=None)\n",
    "\n",
    "def evaluate(ner_model, examples):\n",
    "    scorer = Scorer()\n",
    "    for input_, annot in examples:\n",
    "        doc_gold_text = ner_model.make_doc(input_)\n",
    "        gold = GoldParse(doc_gold_text, entities=annot)\n",
    "        pred_value = ner_model(input_)\n",
    "        scorer.score(pred_value, gold)\n",
    "    return scorer.scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "boundary = re.compile('^[0-9]$')\n",
    "\n",
    "\n",
    "def custom_seg(doc):\n",
    "    prev = doc[0].text\n",
    "    length = len(doc)\n",
    "    for index, token in enumerate(doc):\n",
    "        is_number = token.text == '.' and boundary.match(prev) and index != (length - 1)\n",
    "        if is_number or token.text in [':', ';', ',', '/', '*'] or not token.is_punct:\n",
    "            next_t = index + 1\n",
    "            while next_t < length:\n",
    "                doc[next_t].sent_start = False\n",
    "                if doc[next_t].is_space:\n",
    "                    next_t += 1\n",
    "                else:\n",
    "                    break\n",
    "        prev = token.text\n",
    "    return doc\n",
    "\n",
    "\n",
    "CUSTOM_SEG = 'custom_seg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "with jsonlines.open('dataset.jsonl') as reader:\n",
    "    data = [obj for obj in reader]\n",
    "\n",
    "train_data = [(row['text'], {'entities': row['labels']}) for row in data[:150]]\n",
    "test_data = [(row['text'], row['labels']) for row in data[150:]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['tagger', 'custom_seg', 'parser', 'ner']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp = spacy.load('de_core_news_lg')\n",
    "if CUSTOM_SEG in nlp.pipe_names:\n",
    "    nlp.remove_pipe(CUSTOM_SEG)\n",
    "nlp.add_pipe(custom_seg, name=CUSTOM_SEG, before='parser')\n",
    "\n",
    "nlp.tokenizer = create_custom_tokenizer(nlp)\n",
    "nlp.pipe_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"ner\" not in nlp.pipe_names:\n",
    "    ner = nlp.create_pipe(\"ner\")\n",
    "    nlp.add_pipe(ner)\n",
    "else:\n",
    "    ner = nlp.get_pipe(\"ner\")\n",
    "\n",
    "for _, annotations in train_data:\n",
    "    for ent in annotations.get(\"entities\"):\n",
    "        ner.add_label(ent[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = nlp.resume_training()\n",
    "move_names = list(ner.move_names)\n",
    "pipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"]\n",
    "other_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/30 [00:00<?, ?it/s]2020-12-30 14:12:36.339 | INFO     | __main__:<module>:13 - Losses: {'ner': 5658.7229177059635}\n",
      "  3%|▎         | 1/30 [00:13<06:29, 13.43s/it]2020-12-30 14:12:49.866 | INFO     | __main__:<module>:13 - Losses: {'ner': 5277.909235156979}\n",
      "  7%|▋         | 2/30 [00:26<06:16, 13.46s/it]2020-12-30 14:13:02.545 | INFO     | __main__:<module>:13 - Losses: {'ner': 5027.828117396051}\n",
      " 10%|█         | 3/30 [00:39<05:57, 13.24s/it]2020-12-30 14:13:14.864 | INFO     | __main__:<module>:13 - Losses: {'ner': 5008.63415780256}\n",
      " 13%|█▎        | 4/30 [00:51<05:36, 12.96s/it]2020-12-30 14:13:25.515 | INFO     | __main__:<module>:13 - Losses: {'ner': 5310.320927485401}\n",
      " 17%|█▋        | 5/30 [01:02<05:06, 12.27s/it]2020-12-30 14:13:34.030 | INFO     | __main__:<module>:13 - Losses: {'ner': 4487.8864158419465}\n",
      " 20%|██        | 6/30 [01:11<04:27, 11.15s/it]2020-12-30 14:13:42.737 | INFO     | __main__:<module>:13 - Losses: {'ner': 5335.494804062104}\n",
      " 23%|██▎       | 7/30 [01:19<03:59, 10.40s/it]2020-12-30 14:13:51.873 | INFO     | __main__:<module>:13 - Losses: {'ner': 4261.780591227766}\n",
      " 27%|██▋       | 8/30 [01:29<03:40, 10.03s/it]2020-12-30 14:14:00.688 | INFO     | __main__:<module>:13 - Losses: {'ner': 6133.214893757432}\n",
      " 30%|███       | 9/30 [01:37<03:22,  9.66s/it]2020-12-30 14:14:09.293 | INFO     | __main__:<module>:13 - Losses: {'ner': 5991.420528140856}\n",
      " 33%|███▎      | 10/30 [01:46<03:06,  9.35s/it]2020-12-30 14:14:16.907 | INFO     | __main__:<module>:13 - Losses: {'ner': 4237.63167136608}\n",
      " 37%|███▋      | 11/30 [01:53<02:47,  8.82s/it]2020-12-30 14:14:24.386 | INFO     | __main__:<module>:13 - Losses: {'ner': 4528.1549442940395}\n",
      " 40%|████      | 12/30 [02:01<02:31,  8.42s/it]2020-12-30 14:14:32.012 | INFO     | __main__:<module>:13 - Losses: {'ner': 3962.808051294794}\n",
      " 43%|████▎     | 13/30 [02:09<02:19,  8.18s/it]2020-12-30 14:14:39.390 | INFO     | __main__:<module>:13 - Losses: {'ner': 4973.286634948681}\n",
      " 47%|████▋     | 14/30 [02:16<02:07,  7.94s/it]2020-12-30 14:14:46.751 | INFO     | __main__:<module>:13 - Losses: {'ner': 7735.636037379038}\n",
      " 50%|█████     | 15/30 [02:23<01:56,  7.78s/it]2020-12-30 14:14:53.881 | INFO     | __main__:<module>:13 - Losses: {'ner': 4190.715408224323}\n",
      " 53%|█████▎    | 16/30 [02:30<01:46,  7.58s/it]2020-12-30 14:15:00.716 | INFO     | __main__:<module>:13 - Losses: {'ner': 3954.3815463783812}\n",
      " 57%|█████▋    | 17/30 [02:37<01:35,  7.35s/it]2020-12-30 14:15:07.562 | INFO     | __main__:<module>:13 - Losses: {'ner': 4071.9539576710386}\n",
      " 60%|██████    | 18/30 [02:44<01:26,  7.21s/it]2020-12-30 14:15:14.495 | INFO     | __main__:<module>:13 - Losses: {'ner': 4054.553386415237}\n",
      " 63%|██████▎   | 19/30 [02:51<01:18,  7.11s/it]2020-12-30 14:15:21.359 | INFO     | __main__:<module>:13 - Losses: {'ner': 4005.8081625123436}\n",
      " 67%|██████▋   | 20/30 [02:58<01:10,  7.05s/it]2020-12-30 14:15:28.484 | INFO     | __main__:<module>:13 - Losses: {'ner': 4159.751509661914}\n",
      " 70%|███████   | 21/30 [03:05<01:03,  7.08s/it]2020-12-30 14:15:35.431 | INFO     | __main__:<module>:13 - Losses: {'ner': 5309.695045622313}\n",
      " 73%|███████▎  | 22/30 [03:12<00:56,  7.03s/it]2020-12-30 14:15:41.995 | INFO     | __main__:<module>:13 - Losses: {'ner': 6475.986975328997}\n",
      " 77%|███████▋  | 23/30 [03:19<00:48,  6.88s/it]2020-12-30 14:15:48.557 | INFO     | __main__:<module>:13 - Losses: {'ner': 4200.4624802421895}\n",
      " 80%|████████  | 24/30 [03:25<00:40,  6.79s/it]2020-12-30 14:15:55.296 | INFO     | __main__:<module>:13 - Losses: {'ner': 4003.2879832088656}\n",
      " 83%|████████▎ | 25/30 [03:32<00:33,  6.78s/it]2020-12-30 14:16:01.889 | INFO     | __main__:<module>:13 - Losses: {'ner': 4154.712992318135}\n",
      " 87%|████████▋ | 26/30 [03:38<00:26,  6.71s/it]2020-12-30 14:16:08.283 | INFO     | __main__:<module>:13 - Losses: {'ner': 6520.095487763174}\n",
      " 90%|█████████ | 27/30 [03:45<00:19,  6.62s/it]2020-12-30 14:16:14.637 | INFO     | __main__:<module>:13 - Losses: {'ner': 4734.871095951414}\n",
      " 93%|█████████▎| 28/30 [03:51<00:13,  6.54s/it]2020-12-30 14:16:20.886 | INFO     | __main__:<module>:13 - Losses: {'ner': 4449.5169668148155}\n",
      " 97%|█████████▋| 29/30 [03:58<00:06,  6.46s/it]2020-12-30 14:16:27.170 | INFO     | __main__:<module>:13 - Losses: {'ner': 4751.129628352821}\n",
      "100%|██████████| 30/30 [04:04<00:00,  8.14s/it]\n"
     ]
    }
   ],
   "source": [
    "with nlp.disable_pipes(*other_pipes), warnings.catch_warnings():\n",
    "\n",
    "    warnings.filterwarnings(\"once\", category=UserWarning, module='spacy')\n",
    "    sizes = compounding(1.0, 16.0, 1.001)\n",
    "\n",
    "    for _ in tqdm(range(30)):\n",
    "        random.shuffle(train_data)\n",
    "        batches = minibatch(train_data, size=sizes)\n",
    "        losses = {}\n",
    "        for batch in batches:\n",
    "            texts, annotations = zip(*batch)\n",
    "            nlp.update(texts, annotations, sgd=optimizer, drop=0.2, losses=losses)\n",
    "        logger.info(f\"Losses: {losses}\")\n",
    "            \n",
    "        scores.append(evaluate(nlp, test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0 98.0 98.98989898989899\n"
     ]
    }
   ],
   "source": [
    "#last iteration score\n",
    "index = -1\n",
    "print(scores[index]['ents_p'], scores[index]['ents_r'], scores[index]['ents_f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'uas': 0.0,\n",
       " 'las': 0.0,\n",
       " 'las_per_type': {'': {'p': 0.0, 'r': 0.0, 'f': 0.0}},\n",
       " 'ents_p': 100.0,\n",
       " 'ents_r': 98.0,\n",
       " 'ents_f': 98.98989898989899,\n",
       " 'ents_per_type': {'COMPANY_NAME': {'p': 100.0, 'r': 100.0, 'f': 100.0},\n",
       "  'COMPANY_ADDRESS': {'p': 100.0, 'r': 96.0, 'f': 97.95918367346938}},\n",
       " 'tags_acc': 0.0,\n",
       " 'token_acc': 100.0,\n",
       " 'textcat_score': 0.0,\n",
       " 'textcats_per_cat': {}}"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save model\n",
    "nlp.meta['name'] = 'Registration Docs Parser'\n",
    "nlp.meta['version'] = '1'\n",
    "nlp.remove_pipe(CUSTOM_SEG)\n",
    "nlp.to_disk('model/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "## load model\n",
    "nlp = spacy.load('model/')\n",
    "if CUSTOM_SEG in nlp.pipe_names:\n",
    "    nlp.remove_pipe(CUSTOM_SEG)\n",
    "nlp.add_pipe(custom_seg, name=CUSTOM_SEG, before='parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90.9090909090909 90.0 90.45226130653265\n"
     ]
    }
   ],
   "source": [
    "## test data on independent data\n",
    "score = evaluate(nlp, test_data)\n",
    "print(score['ents_p'], score['ents_r'], score['ents_f'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
